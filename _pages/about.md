---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am pursuing doctoral studies at the Electrical Engineering and Computer Science (EECS) Department at MIT, under the supervision of Prof. Antonio Torralba. My research interests are primarily focused on interpretability and generative AI. Check out my [publications](https://joaanna.github.io/publications/)!

Before embarking on my PhD journey, I completed undergraduate studies in Mathematics at the University of Vienna and The Autonomous University of Barcelona. Following this, I gained industry exposure as an AI Engineer at Twenty Billion Neurons, which was subsequently acquired by Qualcomm. Furthering my academic pursuits, I completed a Master of Science (MSc) degree in Computer Science from the University of Oxford and had the opportunity to contribute as a research intern at the Torr Vision Group.

In addition to my academic and professional commitments, I actively participate in the academic community by serving as a reviewer for a number of esteemed conferences and journals, including ICCV (2021, 2023), CVPR (2022, 2023, 2024), ICLR (2022), NeurIPS (2021), ACM FAccT (2024), TPAMI, and IJCV.

------------------
# News

## 2024
- **October** Our work "NewMove: Customizing text-to-video models with novel motions" was accepted at ACCV 2024!
- Our work "Concept sliders: Lora adaptors for precise control in diffusion models" was presented at ECCV 2024!
- We presented our paper "AirLetters: An Open Video Dataset of Characters Drawn in the Air" at HANDS Workshop in ECCV 2024!
- The [work](https://www.dataprovenance.org/consent-in-crisis-paper) in collaboration with the Data Provenance Initiative got accepted at Neurips 2024! 
- **April** Our workshop [Text, Camera, Action!
Frontiers in Controllable Video Generation](https://sites.google.com/view/cvgicml2024) got accepted at the ICML 2024' Conference in Vienna!
- **February** Honoured to be invited as a speaker at the [Rising Stars in AI Symposium 2024
](https://cemse.kaust.edu.sa/ai/aii-symp-2024) in Saudi Arabia!

## 2023
- **December** Our work ["FIND: A Function Description Benchmark for Evaluating Interpretability Methods"](https://multimodal-interpretability.csail.mit.edu/FIND-benchmark/) was accepted at NeurIPS 2023!
- **December** Our work ["Unified Concept Editing in Diffusion Models"](https://unified.baulab.info/) was accepted at WACV 2023!
- **June - August** I had the pleasure to intern at Adobe Research under the supervision of Bryan Russell, Richard Zhang, Josef Sivic and Eli Shechtman.
- **May**: Our paper "Erasing Concepts from Diffusion Models" has been chosen for a spotlight presentation at the [Machine Learning Advances Symposium](https://mlas.mit.edu/).

## 2022
- **July**: Proud to have won the second place of the poster competition at the [International Computer Vision Summer School](https://iplab.dmi.unict.it/icvss2022/CallForPosters) .
- **June**: Our paper "Disentangling visual and written concepts in CLIP" got awarded an Oral  (~4% acceptance rate) at CVPR 22' in New Orleans.
- **June-August** I was fortunate to intern at Netflix Research under the supervision of Mahdi M Kalayeh





<div><h1>Publications</h1></div>
<div id="publications">
    <article class="pub">
        <a class="pub_image"><img src="images/teaser.png" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">Disentangling visual and written concepts in CLIP</h3>
            <h4 class="authors" style="font-weight: normal;">
                 <b>Joanna Materzynska</b>, Antonio Torralba, David Bau
            </h4>
            <p class="conference">CVPR 2022 <b>(Oral)</b></p>
            [<a href="https://arxiv.org/abs/2206.07835">paper</a>]
            [<a href="https://github.com/joaanna/disentangling_spelling_in_clip">code</a>]
        </div>
    </article>

    <article class="pub">
        <a class="pub_image"><img src="images/reenacting.png" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">Re-enacting video shots with fictional characters</h3>
            <h4 class="authors" style="font-weight: normal;">
                 <b>Joanna Materzynska</b>, David Bau, Antonio Torralba
            </h4>
            <p class="conference">ICCV 2021 CVEU Workshop</p>
            [<a href="https://cveu.github.io/src/re_enacting.pdf">paper</a>]
        </div>
    </article>
    

      <article class="pub">
        <a class="pub_image"><img src="images/smthelse.png" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">Something-else: Compositional action recognition with spatial-temporal interaction networks</h3>
            <h4 class="authors" style="font-weight: normal;">
                <b>Joanna Materzyńska</b> Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, Trevor Darrell

            </h4>
            <p class="conference">CVPR 2020</p>
            [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Materzynska_Something-Else_Compositional_Action_Recognition_With_Spatial-Temporal_Interaction_Networks_CVPR_2020_paper.pdf">paper</a>]
            [<a href="https://github.com/joaanna/something_else">code</a>]
            [<a href="https://joaanna.github.io/something_else/">website</a>]
        </div>
    </article>

      <article class="pub">
        <a class="pub_image"><img src="images/jester.jpeg" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">The jester dataset: A large-scale video dataset of human gestures</h3>
            <h4 class="authors" style="font-weight: normal;">
                <b>Joanna Materzyńska</b> Guillaume Berger, Ingo Bax, Roland Memisevic
            </h4>
            <p class="conference">ICCVW 2019 HANDS Workshop</p>
            [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf">paper</a>]
            [<a href="https://www.qualcomm.com/developer/software/jester-dataset">dataset</a>]
        </div>
    </article>

    <article class="pub">
        <a class="pub_image"><img src="images/smthsmth.png" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">The "something something" video database for learning and evaluating visual common sense</h3>
            <h4 class="authors" style="font-weight: normal;">
                Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, <b>Joanna Materzyńska</b>, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic
            </h4>
            <p class="conference">ICCV 2017</p>
            [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf">paper</a>]
            [<a href="https://www.qualcomm.com/developer/software/something-something-v-2-dataset">dataset</a>]
        </div>
    </article>

    <article class="pub">
        <a class="pub_image"><img src="images/synthia.jpg" style="background-color: transparent;"></a>
        <div class="pub_text">
            <h3 class="papertitle">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</h3>
            <h4 class="authors" style="font-weight: normal;">
                German Ros, Laura Sellart, <b>Joanna Materzyńska</b>, David Vazquez, Antonio M Lopez
            </h4>
            <p class="conference">CVPR 2016</p>
            [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf">paper</a>]
        </div>
    </article>
</div>



------------------
### To learn more about Captioning & Accessibility resources at the MIT please refer to the [website](https://accessibility.mit.edu/).
